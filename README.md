# AI Music Composer with Emotion Control

## ğŸ“Œ Project Overview
AI Music Composer with Emotion Control is a Python-based project that uses Generative AI to create music compositions based on emotional cues. Using TensorFlow and MIDI processing, this system generates emotionally expressive music, making it suitable for applications in games, films, or virtual experiences.

---

## ğŸ§‘â€ğŸ’» Features
- Emotion-driven music composition
- LSTM-based generative AI model
- Emotion classification for dynamic music generation
- MIDI file processing and generation
- Real-time emotion-based soundtrack creation

---

## ğŸ› ï¸ Tech Stack
- **Programming Language:** Python
- **Frameworks:** TensorFlow, Keras
- **Music Processing:** Mido, Music21
- **Data Management:** JSON
- **Modeling:** LSTM Neural Networks

---

## ğŸš€ Project Structure
```
AI_Music_Composer/
â”‚
â”œâ”€â”€ main.py                          # Entry point for running the application
â”œâ”€â”€ config.py                        # Configuration settings for model, emotions, and MIDI
â”œâ”€â”€ requirements.txt                 # List of dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ midi_files/                  # Collection of MIDI files for training
â”‚   â””â”€â”€ emotions.json                # Mapping of MIDI files to emotions
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ emotion_composer.py          # Core generative model
â”‚   â”œâ”€â”€ lstm_model.py                # LSTM-based generative AI for music composition
â”‚   â””â”€â”€ emotion_classifier.py        # Model to classify and interpret emotions
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ midi_processor.py            # MIDI file parsing and data preprocessing
â”‚   â”œâ”€â”€ data_loader.py               # Loads and prepares data for training
â”‚   â””â”€â”€ feature_extractor.py         # Extracts relevant features from MIDI data
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_composer.py            # Train the music generation model
â”‚   â”œâ”€â”€ train_classifier.py          # Train the emotion classifier model
â”‚   â””â”€â”€ evaluation.py                # Evaluate model performance
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ midi_utils.py                # Utilities for MIDI manipulation
â”‚   â”œâ”€â”€ audio_converter.py           # Convert MIDI to audio (optional)
â”‚   â””â”€â”€ emotion_mapping.py           # Helper functions for emotion mapping
â”‚
â””â”€â”€ outputs/
    â”œâ”€â”€ generated_music/             # Generated MIDI or audio files
    â””â”€â”€ logs/                        # Logs and model performance metrics
```

---

## ğŸ§‘â€ğŸ« Installation
1. Clone the repository:
```bash
git clone https://github.com/Nikosane/ AI-Music-Composer-with-Emotion-Control.git
cd  AI-Music-Composer-with-Emotion-Control
```
2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```
3. Install the required packages:
```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Usage
1. Run the main application:
```bash
python main.py
```
2. Choose an emotion input from available options (e.g., Happy, Sad, Angry, Calm).
3. The AI will generate a MIDI file based on the emotional cue.
4. Find the output in the `outputs/generated_music/` directory.

---

## ğŸ“Š Training
1. Prepare MIDI data and label emotions in `data/emotions.json`.
2. Train the emotion classifier:
```bash
python training/train_classifier.py
```
3. Train the generative model:
```bash
python training/train_composer.py
```
4. Evaluate models:
```bash
python training/evaluation.py
```

---

## ğŸ›ï¸ Contributing
Contributions are welcome! If you'd like to contribute, follow these steps:
1. Fork the repository.
2. Create a new branch.
3. Commit your changes.
4. Submit a pull request.

---

## ğŸ“§ Contact
For questions or issues, reach out via GitHub Issues or contact the author at [niteshkotian3@gmail.com](mailto:niteshkotian3@gmail.com).

Happy composing! ğŸµ

